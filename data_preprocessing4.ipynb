{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgg91zRcJT9JBTghI3iKyA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Giuse1093/CSI_Project4/blob/main/data_preprocessing4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9zeAbUDZPEU"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 1) SETUP LIBRERIE E UNZIP\n",
        "# ============================================================\n",
        "!pip install -q flwr tensorflow pandas numpy scipy scikit-learn\n",
        "!pip install -q \"cryptography>=44.0.1,<45.0.0\"\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import flwr as fl\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"Flower OK:\", fl.__version__)\n",
        "print(\"TensorFlow OK:\", tf.__version__)\n",
        "\n",
        "# Decomprimiamo il file di training se non esiste già la cartella\n",
        "if not os.path.exists(\"CSV_train\") and os.path.exists(\"CSV_train.zip\"):\n",
        "    print(\"Decompressione CSV_train.zip in corso...\")\n",
        "    with zipfile.ZipFile(\"CSV_train.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"CSV_train\")\n",
        "    print(\"Fatto!\")\n",
        "elif not os.path.exists(\"CSV_train.zip\"):\n",
        "    print(\"ATTENZIONE: Carica il file 'CSV_train.zip' su Colab!\")\n",
        "else:\n",
        "    print(\"Cartella CSV_train già presente.\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2) PREPROCESSING: PULIZIA + FEATURES DA SERIE TEMPORALI\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "from scipy.fft import fft\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Colonne contenenti le serie temporali\n",
        "TS_COLS = ['hr_time_series', 'resp_time_series', 'stress_time_series']\n",
        "\n",
        "def clean_series_interpolation(series_str):\n",
        "    \"\"\"\n",
        "    1. Parsing stringa -> lista.\n",
        "    2. Valori <=0 (errori sensore: -1, -2, 0) -> NaN.\n",
        "    3. Interpolazione lineare.\n",
        "    4. Riempimento NaN rimanenti con la mediana.\n",
        "    \"\"\"\n",
        "    if not isinstance(series_str, str):\n",
        "        return np.array([])\n",
        "\n",
        "    try:\n",
        "        data = np.array(ast.literal_eval(series_str), dtype=float)\n",
        "\n",
        "        # Serie troppo corte = poco informative\n",
        "        if len(data) < 5:\n",
        "            return np.array([])\n",
        "\n",
        "        # Sostituisco valori \"errati\" con NaN\n",
        "        data[data <= 0] = np.nan\n",
        "\n",
        "        s_data = pd.Series(data)\n",
        "        s_data = s_data.interpolate(method='linear', limit_direction='both')\n",
        "\n",
        "        # Se restano NaN, uso la mediana\n",
        "        if s_data.isna().any():\n",
        "            s_data = s_data.fillna(s_data.median())\n",
        "\n",
        "        return s_data.values\n",
        "    except Exception:\n",
        "        return np.array([])\n",
        "\n",
        "def extract_features_datasets_py(clean_data):\n",
        "    \"\"\"\n",
        "    Estrae: Mean, Std, Percentili (25, 50, 75), Energia Spettrale.\n",
        "    \"\"\"\n",
        "    if len(clean_data) == 0:\n",
        "        return [0.0] * 6\n",
        "\n",
        "    mean = np.mean(clean_data)\n",
        "    std = np.std(clean_data)\n",
        "    p25 = np.percentile(clean_data, 25)\n",
        "    p50 = np.percentile(clean_data, 50)\n",
        "    p75 = np.percentile(clean_data, 75)\n",
        "\n",
        "    f_transform = fft(clean_data)\n",
        "    spectral_energy = np.sum(np.abs(f_transform) ** 2) / len(clean_data)\n",
        "\n",
        "    return [mean, std, p25, p50, p75, spectral_energy]\n",
        "\n",
        "def process_dataframe(df):\n",
        "    \"\"\"\n",
        "    Da dataframe grezzo -> dataframe di feature numeriche da TS_COLS.\n",
        "    \"\"\"\n",
        "    extracted_features = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        row_feats = []\n",
        "        for col in TS_COLS:\n",
        "            clean_seq = clean_series_interpolation(row.get(col, np.nan))\n",
        "            feats = extract_features_datasets_py(clean_seq)\n",
        "            row_feats.extend(feats)\n",
        "        extracted_features.append(row_feats)\n",
        "\n",
        "    prefixes = ['hr', 'resp', 'stress']\n",
        "    suffixes = ['mean', 'std', 'p25', 'p50', 'p75', 'energy']\n",
        "    col_names = [f\"{p}_{s}\" for p in prefixes for s in suffixes]\n",
        "\n",
        "    return pd.DataFrame(extracted_features, columns=col_names)\n",
        "\n",
        "def clean_main_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Pulizia generale del dataframe:\n",
        "    - rimozione righe senza label\n",
        "    - sostituzione sentinel (-1, -2) con NaN sulle colonne numeriche\n",
        "    - drop righe con troppi NaN\n",
        "    - imputazione NaN con mediana colonna\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # 1) Rimuovo righe senza label\n",
        "    if 'label' in df.columns:\n",
        "        df = df[df['label'].notna()]\n",
        "\n",
        "    # 2) Sostituisco codici sentinella con NaN su colonne numeriche\n",
        "    sentinel_values = [-1, -2]\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    df[num_cols] = df[num_cols].replace(sentinel_values, np.nan)\n",
        "\n",
        "    # 3) Drop righe con più del 40% di NaN\n",
        "    thresh = int(0.6 * df.shape[1])\n",
        "    df = df.dropna(axis=0, thresh=thresh)\n",
        "\n",
        "    # 4) Imputo i NaN numerici rimanenti con mediana di colonna\n",
        "    for c in num_cols:\n",
        "        median_val = df[c].median()\n",
        "        df[c] = df[c].fillna(median_val)\n",
        "\n",
        "    return df\n",
        "\n",
        "print(\"Preprocessing definito correttamente.\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3) MODELLO NN MIGLIORATO\n",
        "# ============================================================\n",
        "from tensorflow.keras import layers, models, callbacks, losses\n",
        "\n",
        "def get_model(input_dim):\n",
        "    \"\"\"\n",
        "    Rete neurale densa per regressione (0-100), con BatchNorm e Huber loss.\n",
        "    \"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.2),\n",
        "\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.2),\n",
        "\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "\n",
        "        layers.Dense(1, activation='linear')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=losses.Huber(delta=5.0),\n",
        "        metrics=['mae', 'mse']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "print(\"Modello definito.\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4) FEDERATED LEARNING MIGLIORATO (FEDAVG PESATO)\n",
        "# ============================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "TRAIN_ROOT = \"CSV_train\"\n",
        "ROUNDS = 15\n",
        "EPOCHS_PER_CLIENT = 5\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def run_federated_learning_smart():\n",
        "    # 1. Scoperta automatica dei client\n",
        "    client_folders = []\n",
        "    for root, dirs, files in os.walk(TRAIN_ROOT):\n",
        "        csv_files = [f for f in files if f.endswith('.csv') and 'train' in f]\n",
        "        if len(csv_files) > 0:\n",
        "            client_folders.append(root)\n",
        "\n",
        "    if not client_folders:\n",
        "        print(\"❌ ERRORE: Nessun file .csv di training trovato! Verifica il file zip.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"✅ Trovati {len(client_folders)} Gruppi/Client:\")\n",
        "    for c in client_folders:\n",
        "        n_files = len([f for f in os.listdir(c) if f.endswith('.csv')])\n",
        "        print(f\"   - {c} ({n_files} file csv)\")\n",
        "\n",
        "    # 2. Inizializzazione modello globale\n",
        "    first_folder = client_folders[0]\n",
        "    first_csv = [f for f in os.listdir(first_folder) if f.endswith('.csv')][0]\n",
        "\n",
        "    print(\"\\nAnalisi dimensioni input...\")\n",
        "    sample_df = pd.read_csv(os.path.join(first_folder, first_csv), sep=';')\n",
        "    sample_df = clean_main_dataframe(sample_df)\n",
        "    sample_X = process_dataframe(sample_df.head(5))\n",
        "    input_dim = sample_X.shape[1]\n",
        "    print(f\"Dimensioni Feature Input: {input_dim}\")\n",
        "\n",
        "    global_model = get_model(input_dim)\n",
        "    global_weights = global_model.get_weights()\n",
        "\n",
        "    # --- LOOP FEDERATO ---\n",
        "    print(\"\\nInizio Training Federato...\")\n",
        "    for round_num in range(ROUNDS):\n",
        "        print(f\"\\n--- Round {round_num + 1}/{ROUNDS} ---\")\n",
        "        local_weights_list = []\n",
        "        local_losses = []\n",
        "        local_sizes = []\n",
        "\n",
        "        for client_folder in client_folders:\n",
        "            try:\n",
        "                group_csvs = [\n",
        "                    os.path.join(client_folder, f)\n",
        "                    for f in os.listdir(client_folder)\n",
        "                    if f.endswith('.csv')\n",
        "                ]\n",
        "\n",
        "                df_list = []\n",
        "                for csv_path in group_csvs:\n",
        "                    try:\n",
        "                        df_temp = pd.read_csv(csv_path, sep=';')\n",
        "                        df_list.append(df_temp)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "                if not df_list:\n",
        "                    continue\n",
        "\n",
        "                df_client = pd.concat(df_list, ignore_index=True)\n",
        "                df_client = clean_main_dataframe(df_client)\n",
        "\n",
        "                if 'label' not in df_client.columns:\n",
        "                    continue\n",
        "\n",
        "                # Feature + target\n",
        "                X_client = process_dataframe(df_client)\n",
        "                y_client = df_client['label'].values\n",
        "\n",
        "                # Skip se pochi dati\n",
        "                if len(X_client) < 10:\n",
        "                    continue\n",
        "\n",
        "                # Split train/val locale\n",
        "                X_train, X_val, y_train, y_val = train_test_split(\n",
        "                    X_client, y_client, test_size=0.2, random_state=42\n",
        "                )\n",
        "                n_train = len(X_train)\n",
        "                if n_train < 5:\n",
        "                    continue\n",
        "\n",
        "                # Scaling locale\n",
        "                scaler = StandardScaler()\n",
        "                X_train_scaled = scaler.fit_transform(X_train)\n",
        "                X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "                # Modello locale\n",
        "                client_model = get_model(input_dim)\n",
        "                client_model.set_weights(global_weights)\n",
        "\n",
        "                es = callbacks.EarlyStopping(\n",
        "                    monitor='val_loss',\n",
        "                    patience=2,\n",
        "                    restore_best_weights=True\n",
        "                )\n",
        "\n",
        "                history = client_model.fit(\n",
        "                    X_train_scaled, y_train,\n",
        "                    validation_data=(X_val_scaled, y_val),\n",
        "                    epochs=EPOCHS_PER_CLIENT,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    verbose=0,\n",
        "                    callbacks=[es]\n",
        "                )\n",
        "\n",
        "                last_hist = history.history\n",
        "                print(f\"   Client {os.path.basename(client_folder)} \"\n",
        "                      f\"val_mae={last_hist['val_mae'][-1]:.2f}, \"\n",
        "                      f\"val_loss={last_hist['val_loss'][-1]:.2f}\")\n",
        "\n",
        "                local_weights_list.append(client_model.get_weights())\n",
        "                local_losses.append(last_hist['val_loss'][-1])\n",
        "                local_sizes.append(n_train)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   Errore nel gruppo {os.path.basename(client_folder)}: {e}\")\n",
        "\n",
        "        # Aggregazione (FedAvg pesato)\n",
        "        if local_weights_list:\n",
        "            total_samples = np.sum(local_sizes)\n",
        "            avg_weights = []\n",
        "            for layer_idx in range(len(global_weights)):\n",
        "                layer_stack = np.stack(\n",
        "                    [\n",
        "                        w[layer_idx] * (n / total_samples)\n",
        "                        for w, n in zip(local_weights_list, local_sizes)\n",
        "                    ],\n",
        "                    axis=0\n",
        "                )\n",
        "                avg_weights.append(np.sum(layer_stack, axis=0))\n",
        "\n",
        "            global_weights = avg_weights\n",
        "            global_model.set_weights(global_weights)\n",
        "\n",
        "            avg_loss = np.average(local_losses, weights=local_sizes)\n",
        "            print(f\"   >>> Loss di Validazione Media (Global pesata): {avg_loss:.3f}\")\n",
        "        else:\n",
        "            print(\"   Nessun client ha completato il training in questo round.\")\n",
        "\n",
        "    return global_model\n",
        "\n",
        "# ESECUZIONE TRAINING FEDERATO\n",
        "final_model = run_federated_learning_smart()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5) PREDIZIONI SU x_test.csv E CREAZIONE submission.csv\n",
        "# ============================================================\n",
        "if final_model is not None:\n",
        "    print(\"\\nGenerazione predizioni per x_test.csv...\")\n",
        "\n",
        "    df_test = pd.read_csv(\"x_test.csv\", sep=';')\n",
        "\n",
        "    # Pulizia base sul test (senza label)\n",
        "    df_test_clean = df_test.copy()\n",
        "    sentinel_values = [-1, -2]\n",
        "    num_cols_test = df_test_clean.select_dtypes(include=[np.number]).columns\n",
        "    df_test_clean[num_cols_test] = df_test_clean[num_cols_test].replace(sentinel_values, np.nan)\n",
        "    for c in num_cols_test:\n",
        "        median_val = df_test_clean[c].median()\n",
        "        df_test_clean[c] = df_test_clean[c].fillna(median_val)\n",
        "\n",
        "    # Feature Extraction\n",
        "    X_test_extracted = process_dataframe(df_test_clean)\n",
        "\n",
        "    # Normalizzazione (fit sul test per semplicità)\n",
        "    scaler_test = StandardScaler()\n",
        "    X_test_scaled = scaler_test.fit_transform(X_test_extracted)\n",
        "\n",
        "    # Predizione\n",
        "    predictions = final_model.predict(X_test_scaled)\n",
        "\n",
        "    # Creazione CSV di submission\n",
        "    submission = pd.DataFrame({\n",
        "        'id': df_test['id'],\n",
        "        'label': predictions.flatten()\n",
        "    })\n",
        "\n",
        "    submission.to_csv('submission.csv', index=False)\n",
        "    print(\"File 'submission.csv' pronto per il download!\")\n",
        "    print(submission.head())\n",
        "else:\n",
        "    print(\"Training fallito, nessun modello generato.\")\n"
      ]
    }
  ]
}