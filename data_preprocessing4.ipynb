{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXXARbArfTINTYKf6wzD+T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Giuse1093/CSI_Project4/blob/main/data_preprocessing4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r9zeAbUDZPEU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "a719f983-6f72-433f-d99e-ccf0a275cab5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (ipython-input-2663707233.py, line 107)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2663707233.py\"\u001b[0;36m, line \u001b[0;32m107\u001b[0m\n\u001b[0;31m    f_transform = fft(clean_data)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 1) SETUP LIBRERIE\n",
        "# ============================================================\n",
        "!pip install -q flwr tensorflow pandas numpy scipy scikit-learn\n",
        "!pip install -q \"cryptography>=44.0.1,<45.0.0\"\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import flwr as fl\n",
        "import tensorflow as tf\n",
        "print(\"Flower OK:\", fl.__version__)\n",
        "print(\"TensorFlow OK:\", tf.__version__)\n",
        "\n",
        "if not os.path.exists(\"CSV_train\") and os.path.exists(\"CSV_train.zip\"):\n",
        "    print(\"Decompressione CSV_train.zip in corso...\")\n",
        "    with zipfile.ZipFile(\"CSV_train.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"CSV_train\")\n",
        "    print(\"Fatto!\")\n",
        "elif not os.path.exists(\"CSV_train.zip\"):\n",
        "    print(\"ATTENZIONE: Carica il file 'CSV_train.zip' su Colab!\")\n",
        "else:\n",
        "    print(\"Cartella CSV_train già presente.\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "from scipy.fft import fft\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models, callbacks, losses\n",
        "\n",
        "# ============================================================\n",
        "# 2) PREPROCESSING ROBUSTO\n",
        "# ============================================================\n",
        "\n",
        "TS_COLS = ['hr_time_series', 'resp_time_series', 'stress_time_series']\n",
        "\n",
        "# Feature statiche forti da estrarre\n",
        "STATIC_COLS = [\n",
        "    'sleep_sleepTimeSeconds',\n",
        "    'sleep_deepSleepSeconds',\n",
        "    'sleep_lightSleepSeconds',\n",
        "    'sleep_remSleepSeconds',\n",
        "    'sleep_awakeSleepSeconds',\n",
        "    'sleep_averageRespirationValue',\n",
        "    'sleep_avgHeartRate',\n",
        "    'sleep_avgSleepStress',\n",
        "    'hr_restingHeartRate',\n",
        "    'act_totalCalories',\n",
        "    'act_activeTime'\n",
        "]\n",
        "\n",
        "def clean_series_interpolation(series_str):\n",
        "    \"\"\"\n",
        "    Pulizia robusta della serie temporale:\n",
        "    - Parsing sicuro\n",
        "    - NaN per valori invalidi\n",
        "    - Interpolazione lineare\n",
        "    - Riempimento con mediana (non 0)\n",
        "    \"\"\"\n",
        "    if not isinstance(series_str, str):\n",
        "        return np.array([])\n",
        "\n",
        "    try:\n",
        "        data = np.array(ast.literal_eval(series_str), dtype=float)\n",
        "\n",
        "        # Serie troppo corte sono inutili\n",
        "        if len(data) < 5:\n",
        "            return np.array([])\n",
        "\n",
        "        # Valori invalidi -> NaN\n",
        "        data[data <= 0] = np.nan\n",
        "\n",
        "        s_data = pd.Series(data)\n",
        "\n",
        "        # Interpolazione lineare (preserva trend)\n",
        "        s_data = s_data.interpolate(method='linear', limit_direction='both')\n",
        "\n",
        "        # Se restano NaN, usa mediana della serie (robusto)\n",
        "        if s_data.isna().any():\n",
        "            median_val = s_data.median()\n",
        "            if not np.isfinite(median_val):\n",
        "                return np.array([])\n",
        "            s_data = s_data.fillna(median_val)\n",
        "\n",
        "        return s_data.values\n",
        "    except Exception:\n",
        "        return np.array([])\n",
        "\n",
        "def extract_features_ts(clean_data):\n",
        "    \"\"\"\n",
        "    Estrae features da serie temporale (stile prof):\n",
        "    Mean, Std, P25, P50, P75, Energia Spettrale FFT\n",
        "    \"\"\"\n",
        "    if len(clean_data) == 0:\n",
        "        return [0.0] * 6\n",
        "\n",
        "    mean = float(np.mean(clean_data))\n",
        "    std = float(np.std(clean_data))\n",
        "    p25 = float(np.percentile(clean_data, 25))\n",
        "    p50 = float(np.percentile(clean_data, 50))\n",
        "    p75 = float(np.percentile(clean_data, 75))\n",
        "\n",
        "    f_transform = fft(clean_data)\n",
        "    spectral_energy = float(np.sum(np.abs(f_transform) ** 2) / len(clean_data))\n",
        "\n",
        "    # Sanity check\n",
        "    vals = [mean, std, p25, p50, p75, spectral_energy]\n",
        "    vals = [0.0 if (not np.isfinite(v)) else v for v in vals]\n",
        "\n",
        "    return vals\n",
        "\n",
        "def clean_main_dataframe(df: pd.DataFrame, is_train: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Pulizia del dataframe principale:\n",
        "    - Rimuove righe senza label (se train)\n",
        "    - Sostituisce sentinelle (-1, -2) con NaN\n",
        "    - Droppa righe con troppi NaN\n",
        "    - Imputa NaN con mediana per colonna\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Rimuovi righe senza label\n",
        "    if is_train and 'label' in df.columns:\n",
        "        df = df[df['label'].notna()]\n",
        "        # Clipping label nel range realistico [0, 100]\n",
        "        df = df[df['label'].between(0, 100)]\n",
        "\n",
        "    # Sostituisci sentinelle\n",
        "    sentinel_values = [-1, -2]\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    for col in num_cols:\n",
        "        df[col] = df[col].replace(sentinel_values, np.nan)\n",
        "\n",
        "    # Droppa righe con troppi NaN (>40%)\n",
        "    thresh = int(0.6 * df.shape[1])\n",
        "    df = df.dropna(axis=0, thresh=thresh)\n",
        "\n",
        "    # Imputa NaN con mediana per colonna\n",
        "    for col in num_cols:\n",
        "        if col in df.columns:\n",
        "            median_val = df[col].median()\n",
        "            if np.isfinite(median_val):\n",
        "                df[col] = df[col].fillna(median_val)\n",
        "            else:\n",
        "                df[col] = df[col].fillna(0.0)\n",
        "\n",
        "    return df\n",
        "\n",
        "def process_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Estrae feature da series temporali + feature statiche.\n",
        "    Output: DataFrame con ~29 feature (18 TS + 11 statiche)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # === FEATURES DA SERIE TEMPORALI ===\n",
        "    ts_features = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        row_feats = []\n",
        "\n",
        "        for col in TS_COLS:\n",
        "            if col not in row.index:\n",
        "                # Se colonna manca, 6 zeri\n",
        "                row_feats.extend([0.0] * 6)\n",
        "            else:\n",
        "                clean_seq = clean_series_interpolation(row[col])\n",
        "                feats = extract_features_ts(clean_seq)\n",
        "                row_feats.extend(feats)\n",
        "\n",
        "        ts_features.append(row_feats)\n",
        "\n",
        "    prefixes = ['hr', 'resp', 'stress']\n",
        "    suffixes = ['mean', 'std', 'p25', 'p50', 'p75', 'energy']\n",
        "    ts_col_names = [f\"{p}_{s}\" for p in prefixes for s in suffixes]\n",
        "\n",
        "    df_ts = pd.DataFrame(ts_features, columns=ts_col_names, index=df.index)\n",
        "\n",
        "    # === FEATURES STATICHE ===\n",
        "    static_cols_present = [c for c in STATIC_COLS if c in df.columns]\n",
        "    df_static = df[static_cols_present].copy() if static_cols_present else pd.DataFrame(index=df.index)\n",
        "\n",
        "    # Sanity check su statiche\n",
        "    if len(df_static) > 0:\n",
        "        df_static = df_static.replace([np.inf, -np.inf], np.nan)\n",
        "        for c in df_static.columns:\n",
        "            median_val = df_static[c].median()\n",
        "            if np.isfinite(median_val):\n",
        "                df_static[c] = df_static[c].fillna(median_val)\n",
        "            else:\n",
        "                df_static[c] = 0.0\n",
        "\n",
        "    # Concatena\n",
        "    X = pd.concat([df_ts, df_static], axis=1)\n",
        "    X = X.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
        "\n",
        "    return X\n",
        "\n",
        "print(\"Preprocessing robusto definito.\")\n",
        "\n",
        "# ============================================================\n",
        "# 3) MODELLO NN MIGLIORATO\n",
        "# ============================================================\n",
        "\n",
        "def get_model(input_dim):\n",
        "    \"\"\"\n",
        "    NN densa con:\n",
        "    - BatchNormalization (stabilizza training)\n",
        "    - Dropout moderato (0.2, non 0.3)\n",
        "    - Huber loss (robusto agli outlier)\n",
        "    - Adam con lr=1e-3\n",
        "    \"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.2),\n",
        "\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.2),\n",
        "\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "\n",
        "        layers.Dense(1, activation='linear')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=losses.Huber(delta=5.0),\n",
        "        metrics=['mae', 'mse']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "print(\"Modello NN definito.\")\n",
        "\n",
        "# ============================================================\n",
        "# 4) FEDERATED LEARNING MIGLIORATO\n",
        "# ============================================================\n",
        "\n",
        "TRAIN_ROOT = \"CSV_train\"\n",
        "ROUNDS = 15\n",
        "EPOCHS_PER_CLIENT = 5\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def run_federated_learning_smart():\n",
        "    \"\"\"\n",
        "    Federated Learning con:\n",
        "    - FedAvg pesato per numero campioni\n",
        "    - Normalizzazione coerente per client\n",
        "    - Early stopping per client\n",
        "    - Logging dettagliato\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Scoperta client\n",
        "    client_folders = []\n",
        "    for root, dirs, files in os.walk(TRAIN_ROOT):\n",
        "        csv_files = [f for f in files if f.endswith('.csv') and 'train' in f]\n",
        "        if len(csv_files) > 0:\n",
        "            client_folders.append(root)\n",
        "\n",
        "    if not client_folders:\n",
        "        print(\"❌ ERRORE: Nessun file CSV trovato!\")\n",
        "        return None\n",
        "\n",
        "    print(f\"✅ Trovati {len(client_folders)} Client:\")\n",
        "    for c in client_folders:\n",
        "        n_files = len([f for f in os.listdir(c) if f.endswith('.csv')])\n",
        "        print(f\"   - {c} ({n_files} file)\")\n",
        "\n",
        "    # 2. Inizializza modello globale\n",
        "    first_folder = client_folders[0]\n",
        "    first_csv = [f for f in os.listdir(first_folder) if f.endswith('.csv')][0]\n",
        "\n",
        "    print(\"\\nAnalisi dimensioni...\")\n",
        "    sample_df = pd.read_csv(os.path.join(first_folder, first_csv), sep=';')\n",
        "    sample_df = clean_main_dataframe(sample_df, is_train=True)\n",
        "    sample_X = process_dataframe(sample_df.head(5))\n",
        "    input_dim = sample_X.shape[1]\n",
        "    print(f\"Feature Input: {input_dim}\")\n",
        "\n",
        "    global_model = get_model(input_dim)\n",
        "    global_weights = global_model.get_weights()\n",
        "\n",
        "    # 3. Loop federato\n",
        "    print(\"\\nInizio Training Federato...\")\n",
        "    for round_num in range(ROUNDS):\n",
        "        print(f\"\\n--- Round {round_num + 1}/{ROUNDS} ---\")\n",
        "\n",
        "        local_weights_list = []\n",
        "        local_losses = []\n",
        "        local_sizes = []\n",
        "\n",
        "        for client_folder in client_folders:\n",
        "            try:\n",
        "                # Carica dati del client\n",
        "                group_csvs = [\n",
        "                    os.path.join(client_folder, f)\n",
        "                    for f in os.listdir(client_folder)\n",
        "                    if f.endswith('.csv')\n",
        "                ]\n",
        "\n",
        "                df_list = []\n",
        "                for csv_path in group_csvs:\n",
        "                    try:\n",
        "                        df_temp = pd.read_csv(csv_path, sep=';')\n",
        "                        df_list.append(df_temp)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "                if not df_list:\n",
        "                    continue\n",
        "\n",
        "                df_client = pd.concat(df_list, ignore_index=True)\n",
        "                df_client = clean_main_dataframe(df_client, is_train=True)\n",
        "\n",
        "                if 'label' not in df_client.columns or len(df_client) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Estrai feature\n",
        "                X_client = process_dataframe(df_client)\n",
        "                y_client = df_client['label'].values\n",
        "\n",
        "                if len(X_client) < 20:\n",
        "                    continue\n",
        "\n",
        "                # Split train/val\n",
        "                X_train, X_val, y_train, y_val = train_test_split(\n",
        "                    X_client, y_client, test_size=0.2, random_state=42\n",
        "                )\n",
        "\n",
        "                n_train = len(X_train)\n",
        "                if n_train < 10:\n",
        "                    continue\n",
        "\n",
        "                # Scaling locale\n",
        "                scaler = StandardScaler()\n",
        "                X_train_scaled = scaler.fit_transform(X_train)\n",
        "                X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "                # Training locale\n",
        "                client_model = get_model(input_dim)\n",
        "                client_model.set_weights(global_weights)\n",
        "\n",
        "                es = callbacks.EarlyStopping(\n",
        "                    monitor='val_loss',\n",
        "                    patience=3,\n",
        "                    restore_best_weights=True\n",
        "                )\n",
        "\n",
        "                history = client_model.fit(\n",
        "                    X_train_scaled, y_train,\n",
        "                    validation_data=(X_val_scaled, y_val),\n",
        "                    epochs=EPOCHS_PER_CLIENT,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    verbose=0,\n",
        "                    callbacks=[es]\n",
        "                )\n",
        "\n",
        "                val_mae = history.history['val_mae'][-1]\n",
        "                val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "                print(f\"   {os.path.basename(client_folder)}: \"\n",
        "                      f\"mae={val_mae:.2f}, loss={val_loss:.2f}, samples={n_train}\")\n",
        "\n",
        "                local_weights_list.append(client_model.get_weights())\n",
        "                local_losses.append(val_loss)\n",
        "                local_sizes.append(n_train)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   Errore in {os.path.basename(client_folder)}: {str(e)[:40]}\")\n",
        "\n",
        "        # Aggregazione (FedAvg pesato)\n",
        "        if local_weights_list:\n",
        "            total_samples = np.sum(local_sizes)\n",
        "            avg_weights = []\n",
        "\n",
        "            for layer_idx in range(len(global_weights)):\n",
        "                weighted_layer = np.zeros_like(global_weights[layer_idx])\n",
        "\n",
        "                for w, n in zip(local_weights_list, local_sizes):\n",
        "                    weighted_layer += w[layer_idx] * (n / total_samples)\n",
        "\n",
        "                avg_weights.append(weighted_layer)\n",
        "\n",
        "            global_weights = avg_weights\n",
        "            global_model.set_weights(global_weights)\n",
        "\n",
        "            avg_loss = np.average(local_losses, weights=local_sizes)\n",
        "            print(f\"   >>> Media pesata Loss: {avg_loss:.3f}\")\n",
        "        else:\n",
        "            print(\"   ⚠️  Nessun client completato.\")\n",
        "\n",
        "    return global_model\n",
        "\n",
        "final_model = run_federated_learning_smart()\n",
        "\n",
        "# ============================================================\n",
        "# 5) PREDIZIONI E SUBMISSION\n",
        "# ============================================================\n",
        "\n",
        "if final_model is not None:\n",
        "    print(\"\\nGenerazione predizioni...\")\n",
        "\n",
        "    df_test = pd.read_csv(\"x_test.csv\", sep=';')\n",
        "\n",
        "    # Pulizia test (senza label)\n",
        "    df_test_clean = clean_main_dataframe(df_test, is_train=False)\n",
        "\n",
        "    # Estrazione feature\n",
        "    X_test_extracted = process_dataframe(df_test_clean)\n",
        "\n",
        "    # Scaling\n",
        "    scaler_test = StandardScaler()\n",
        "    X_test_scaled = scaler_test.fit_transform(X_test_extracted)\n",
        "\n",
        "    # Predizioni\n",
        "    predictions = final_model.predict(X_test_scaled, verbose=0)\n",
        "\n",
        "    # Post-processing\n",
        "    predictions = np.nan_to_num(predictions, nan=75.0, posinf=100.0, neginf=0.0)\n",
        "    predictions = np.clip(predictions, 0, 100)\n",
        "    predictions = np.rint(predictions).astype(int)\n",
        "\n",
        "    # Submission\n",
        "    submission = pd.DataFrame({\n",
        "        'id': df_test_clean['id'], # Changed from df_test['id'] to df_test_clean['id']\n",
        "        'label': predictions.flatten()\n",
        "    })\n",
        "\n",
        "    # Sanity check\n",
        "    assert not submission['label'].isna().any(), \"⚠️  Ci sono NaN!\"\n",
        "    assert (submission['label'] >= 0).all(), \"⚠️  Valori < 0!\"\n",
        "    assert (submission['label'] <= 100).all(), \"⚠️  Valori > 100!\"\n",
        "\n",
        "    submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "    print(\"✅ File 'submission.csv' pronto!\")\n",
        "    print(\"\\nStatistiche predizioni:\")\n",
        "    print(f\"  Min: {submission['label'].min()}\")\n",
        "    print(f\"  Max: {submission['label'].max()}\")\n",
        "    print(f\"  Media: {submission['label'].mean():.1f}\")\n",
        "    print(f\"  Mediana: {submission['label'].median():.1f}\")\n",
        "    print(f\"  Std: {submission['label'].std():.1f}\")\n",
        "    print(\"\\nPrime 10 righe:\")\n",
        "    print(submission.head(10))\n",
        "\n",
        "else:\n",
        "    print(\"❌ Training fallito.\")"
      ]
    }
  ]
}