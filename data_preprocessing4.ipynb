{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRApzfjyPHgBf6CW7FNvWx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Giuse1093/CSI_Project4/blob/main/data_preprocessing4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r9zeAbUDZPEU"
      },
      "outputs": [],
      "source": [
        "# --- CELLA 1: INSTALLAZIONE E SETUP ---\n",
        "!pip install -q flwr pandas numpy scipy scikit-learn\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.fft import fft\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Decompressione Dataset\n",
        "if not os.path.exists(\"CSV_train\") and os.path.exists(\"CSV_train.zip\"):\n",
        "    print(\"Decompressione CSV_train.zip...\")\n",
        "    with zipfile.ZipFile(\"CSV_train.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"CSV_train\")\n",
        "    print(\"Fatto!\")\n",
        "elif not os.path.exists(\"CSV_train.zip\"):\n",
        "    print(\"⚠️ ATTENZIONE: Carica il file 'CSV_train.zip' e 'x_test.csv' su Colab!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELLA 2: PREPROCESSING ---\n",
        "\n",
        "# Colonne che contengono le liste (stringhe) da convertire\n",
        "TS_COLS = ['hr_time_series', 'resp_time_series', 'stress_time_series', 'activity_time_series']\n",
        "\n",
        "def clean_series_interpolation(series_str):\n",
        "    \"\"\"\n",
        "    Ispirato a 'Accelerometer_Preprocessing.ipynb'.\n",
        "    Converte stringa in array, rimuove zeri/errori e interpola i dati mancanti.\n",
        "    \"\"\"\n",
        "    if not isinstance(series_str, str):\n",
        "        return np.array([])\n",
        "\n",
        "    try:\n",
        "        # 1. Parsing sicuro\n",
        "        data = np.array(ast.literal_eval(series_str), dtype=float)\n",
        "\n",
        "        # 2. Pulizia: valori <= 0 spesso sono errori nel Garmin\n",
        "        data[data <= 0] = np.nan\n",
        "\n",
        "        # 3. Interpolazione per mantenere la continuità temporale\n",
        "        s_data = pd.Series(data)\n",
        "        s_data = s_data.interpolate(method='linear', limit_direction='both')\n",
        "        s_data = s_data.fillna(0) # Se tutto è NaN, metti 0\n",
        "\n",
        "        return s_data.values\n",
        "    except:\n",
        "        return np.array([0.0])\n",
        "\n",
        "def calculate_params(matrix):\n",
        "    \"\"\"\n",
        "    Replica ESATTA di 'datasets.py'.\n",
        "    Estrae feature statistiche e spettrali da un array 1D.\n",
        "    \"\"\"\n",
        "    if len(matrix) == 0:\n",
        "        return [0.0] * 6\n",
        "\n",
        "    # Statistiche nel dominio del tempo\n",
        "    mean = np.mean(matrix)\n",
        "    std = np.std(matrix)\n",
        "    perc25 = np.percentile(matrix, 25)\n",
        "    perc50 = np.percentile(matrix, 50) # Mediana\n",
        "    perc75 = np.percentile(matrix, 75)\n",
        "\n",
        "    # Statistiche nel dominio della frequenza (datasets.py usa FFT)\n",
        "    f = fft(matrix)\n",
        "    spEnergy = np.sum(np.abs(f)**2)/len(f)\n",
        "\n",
        "    return [mean, std, perc25, perc50, perc75, spEnergy]\n",
        "\n",
        "def process_dataframe(df):\n",
        "    \"\"\"\n",
        "    Trasforma il dataframe con stringhe in un dataset tabellare di feature numeriche.\n",
        "    \"\"\"\n",
        "    extracted_data = []\n",
        "\n",
        "    # Colonne scalari già presenti (non liste)\n",
        "    scalar_cols = [c for c in df.columns if 'time_series' not in c and c != 'label' and c != 'id' and 'date' not in c]\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        row_features = []\n",
        "\n",
        "        # 1. Aggiungi feature scalari già pronte (es. 'average_stress')\n",
        "        for col in scalar_cols:\n",
        "             # Gestione valori mancanti nelle colonne scalari\n",
        "            val = row[col] if pd.notnull(row[col]) else 0.0\n",
        "            row_features.append(val)\n",
        "\n",
        "        # 2. Elabora le serie temporali\n",
        "        for col in TS_COLS:\n",
        "            if col in df.columns:\n",
        "                clean_seq = clean_series_interpolation(row[col])\n",
        "                feats = calculate_params(clean_seq) # Usa la logica di datasets.py\n",
        "                row_features.extend(feats)\n",
        "\n",
        "        extracted_data.append(row_features)\n",
        "\n",
        "    # Generazione nomi colonne\n",
        "    col_names = list(scalar_cols)\n",
        "    suffixes = ['mean', 'std', 'p25', 'p50', 'p75', 'energy']\n",
        "    for ts_col in TS_COLS:\n",
        "        if ts_col in df.columns:\n",
        "            base_name = ts_col.replace('_time_series', '')\n",
        "            for s in suffixes:\n",
        "                col_names.append(f\"{base_name}_{s}\")\n",
        "\n",
        "    return pd.DataFrame(extracted_data, columns=col_names)\n",
        "\n",
        "print(\"Funzioni di preprocessing pronte.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyutyE601_ZA",
        "outputId": "dde4a8fc-cce9-434d-8740-8ec72dcb06da"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Funzioni di preprocessing pronte.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELLA 3: FEDERATED LEARNING CON RANDOM FOREST ---\n",
        "\n",
        "def run_federated_random_forest():\n",
        "    \"\"\"\n",
        "    Simulazione FL:\n",
        "    1. Ogni client addestra un Random Forest locale sui suoi dati.\n",
        "    2. Il server raccoglie tutti gli stimatori (alberi) dai client.\n",
        "    3. Il Modello Globale è l'unione di tutti gli alberi (Ensemble of Ensembles).\n",
        "    \"\"\"\n",
        "    TRAIN_ROOT = \"CSV_train\"\n",
        "    global_estimators = [] # Qui accumuleremo tutti gli alberi di tutti i client\n",
        "\n",
        "    # Trova le cartelle dei client\n",
        "    client_folders = []\n",
        "    for root, dirs, files in os.walk(TRAIN_ROOT):\n",
        "        csv_files = [f for f in files if f.endswith('.csv')]\n",
        "        if csv_files:\n",
        "            client_folders.append(root)\n",
        "\n",
        "    print(f\"Inizio training su {len(client_folders)} gruppi di client...\")\n",
        "\n",
        "    metrics = []\n",
        "\n",
        "    for i, folder in enumerate(client_folders):\n",
        "        # A. Caricamento e Fusione Dati del Gruppo (Client)\n",
        "        files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.csv')]\n",
        "        df_list = [pd.read_csv(f, sep=';') for f in files]\n",
        "        if not df_list: continue\n",
        "\n",
        "        df_client = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "        # B. Preprocessing\n",
        "        if 'label' not in df_client.columns: continue\n",
        "        y = df_client['label'].values\n",
        "        X = process_dataframe(df_client)\n",
        "\n",
        "        # Split locale per validazione interna (opzionale, solo per vedere come va)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # C. Training Locale\n",
        "        # Usiamo RandomForestRegressor perché il target è continuo (0-100)\n",
        "        # n_estimators basso per client, poi sommati diventeranno tanti\n",
        "        clf = RandomForestRegressor(n_estimators=10, max_depth=10, n_jobs=-1, random_state=42)\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        # Valutazione locale\n",
        "        score = clf.score(X_val, y_val) # R^2 score\n",
        "        mae = mean_absolute_error(y_val, clf.predict(X_val))\n",
        "        metrics.append(mae)\n",
        "        print(f\"  Client {i+1}/{len(client_folders)} - MAE Locale: {mae:.2f}\")\n",
        "\n",
        "        # D. \"Invio\" al server (Accumulo degli alberi)\n",
        "        # In FL reale, invieremmo i parametri. Qui accumuliamo gli alberi.\n",
        "        global_estimators.extend(clf.estimators_)\n",
        "\n",
        "    print(f\"\\nTraining completato. MAE medio locale: {np.mean(metrics):.2f}\")\n",
        "\n",
        "    # Creiamo un modello \"Contenitore\" vuoto per usarlo in predizione\n",
        "    # Deve avere le stesse feature del training\n",
        "    dummy_model = RandomForestRegressor(n_estimators=len(global_estimators), random_state=42)\n",
        "    dummy_model.estimators_ = global_estimators\n",
        "    dummy_model.fit(X_train, y_train) # Fit fittizio solo per inizializzare le classi interne\n",
        "\n",
        "    return dummy_model\n",
        "\n",
        "# ESECUZIONE TRAINING\n",
        "print(\"Avvio Federated Training...\")\n",
        "global_model = run_federated_random_forest()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8STQfTj12Clt",
        "outputId": "691f1aac-3928-410a-a513-1ddfd6fd98dd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avvio Federated Training...\n",
            "Inizio training su 9 gruppi di client...\n",
            "  Client 1/9 - MAE Locale: 9.32\n",
            "  Client 2/9 - MAE Locale: 9.15\n",
            "  Client 3/9 - MAE Locale: 11.42\n",
            "  Client 4/9 - MAE Locale: 12.67\n",
            "  Client 5/9 - MAE Locale: 10.97\n",
            "  Client 6/9 - MAE Locale: 9.63\n",
            "  Client 7/9 - MAE Locale: 11.13\n",
            "  Client 8/9 - MAE Locale: 9.59\n",
            "  Client 9/9 - MAE Locale: 13.08\n",
            "\n",
            "Training completato. MAE medio locale: 10.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELLA 4: PREDIZIONE E SUBMISSION ---\n",
        "if os.path.exists(\"x_test.csv\"):\n",
        "    print(\"\\nGenerazione predizioni su x_test.csv...\")\n",
        "    df_test = pd.read_csv(\"x_test.csv\", sep=';')\n",
        "\n",
        "    # 1. Estrazione Feature (Stessa del training)\n",
        "    X_test = process_dataframe(df_test)\n",
        "\n",
        "    # Align X_test columns with the features the model was trained on\n",
        "    # This addresses the ValueError about mismatched feature names\n",
        "    trained_features = global_model.feature_names_in_\n",
        "\n",
        "    # Identify missing columns in X_test and add them, filling with 0\n",
        "    missing_cols = set(trained_features) - set(X_test.columns)\n",
        "    for c in missing_cols:\n",
        "        X_test[c] = 0.0\n",
        "\n",
        "    # Identify extra columns in X_test and drop them\n",
        "    extra_cols = set(X_test.columns) - set(trained_features)\n",
        "    if extra_cols:\n",
        "        X_test = X_test.drop(columns=list(extra_cols))\n",
        "\n",
        "    # Ensure the order of columns matches the training data\n",
        "    X_test = X_test[trained_features]\n",
        "\n",
        "    # 2. Predizione usando il Super-Ensemble\n",
        "    # Random Forest fa la media delle predizioni di tutti gli alberi accumulati\n",
        "    preds = global_model.predict(X_test)\n",
        "\n",
        "    # 3. Salvataggio\n",
        "    submission = pd.DataFrame({\n",
        "        'id': df_test['id'],\n",
        "        'label': preds\n",
        "    })\n",
        "\n",
        "    submission.to_csv('submission.csv', index=False)\n",
        "    print(\"✅ File 'submission.csv' creato con successo!\")\n",
        "    print(submission.head())\n",
        "else:\n",
        "    print(\"❌ File x_test.csv non trovato.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meGNhwhj2Fnk",
        "outputId": "2b8bc037-636d-49a1-cac9-6b33c2d2880c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generazione predizioni su x_test.csv...\n",
            "✅ File 'submission.csv' creato con successo!\n",
            "   id      label\n",
            "0   0  58.477778\n",
            "1   1  61.811111\n",
            "2   2  60.644444\n",
            "3   3  61.788889\n",
            "4   4  62.122222\n"
          ]
        }
      ]
    }
  ]
}