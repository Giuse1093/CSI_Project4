{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyvfLCTfttl5l4H0mEWC23",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Giuse1093/CSI_Project4/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lIHO5InzaI5Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f134fae9-d01e-4905-bed9-d4c60fb02ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Funzioni di preprocessing definite correttamente.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "from scipy.fft import fft\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "\n",
        "# Ignoriamo warning minori di pandas\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Colonne contenenti le serie temporali\n",
        "TS_COLS = ['hr_time_series', 'resp_time_series', 'stress_time_series']\n",
        "\n",
        "def clean_series_interpolation(series_str):\n",
        "    \"\"\"\n",
        "    Replica la logica di 'Accelerometer_Preprocessing.ipynb':\n",
        "    1. Parsing da stringa a lista.\n",
        "    2. Sostituzione valori errati (<=0) con NaN.\n",
        "    3. Interpolazione Lineare per riempire i buchi mantenendo il trend temporale.\n",
        "    \"\"\"\n",
        "    if not isinstance(series_str, str):\n",
        "        return np.array([])\n",
        "\n",
        "    try:\n",
        "        # 1. Parsing\n",
        "        data = np.array(ast.literal_eval(series_str), dtype=float)\n",
        "\n",
        "        # 2. Rimuovi outlier/errori sensore (-1, -2, 0) impostandoli a NaN\n",
        "        data[data <= 0] = np.nan\n",
        "\n",
        "        # Trasforma in Series per usare interpolate\n",
        "        s_data = pd.Series(data)\n",
        "\n",
        "        # 3. Interpolazione (Linear) - Fondamentale per non perdere la sequenza\n",
        "        s_data = s_data.interpolate(method='linear', limit_direction='both')\n",
        "\n",
        "        # Se restano NaN (es. serie vuota), riempi con 0\n",
        "        s_data = s_data.fillna(0)\n",
        "\n",
        "        return s_data.values\n",
        "    except:\n",
        "        return np.array([])\n",
        "\n",
        "def extract_features_datasets_py(clean_data):\n",
        "    \"\"\"\n",
        "    Replica ESATTA di 'datasets.py' -> 'calculate_params':\n",
        "    Estrae: Mean, Std, Percentili (25, 50, 75), Energia Spettrale.\n",
        "    \"\"\"\n",
        "    if len(clean_data) == 0:\n",
        "        return [0.0] * 6\n",
        "\n",
        "    # Statistiche base\n",
        "    mean = np.mean(clean_data)\n",
        "    std = np.std(clean_data)\n",
        "    p25 = np.percentile(clean_data, 25)\n",
        "    p50 = np.percentile(clean_data, 50) # Mediana\n",
        "    p75 = np.percentile(clean_data, 75)\n",
        "\n",
        "    # Energia Spettrale (FFT) - Come nel file della prof\n",
        "    f_transform = fft(clean_data)\n",
        "    spectral_energy = np.sum(np.abs(f_transform)**2) / len(clean_data)\n",
        "\n",
        "    return [mean, std, p25, p50, p75, spectral_energy]\n",
        "\n",
        "def process_dataframe(df):\n",
        "    \"\"\"\n",
        "    Prende il dataframe grezzo e restituisce un dataframe di feature numeriche.\n",
        "    \"\"\"\n",
        "    extracted_features = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        row_feats = []\n",
        "        for col in TS_COLS:\n",
        "            # A. Pulizia\n",
        "            clean_seq = clean_series_interpolation(row[col])\n",
        "            # B. Estrazione Feature\n",
        "            feats = extract_features_datasets_py(clean_seq)\n",
        "            row_feats.extend(feats)\n",
        "\n",
        "        extracted_features.append(row_feats)\n",
        "\n",
        "    # Creiamo nomi colonne parlanti\n",
        "    col_names = []\n",
        "    prefixes = ['hr', 'resp', 'stress']\n",
        "    suffixes = ['mean', 'std', 'p25', 'p50', 'p75', 'energy']\n",
        "    for p in prefixes:\n",
        "        for s in suffixes:\n",
        "            col_names.append(f\"{p}_{s}\")\n",
        "\n",
        "    return pd.DataFrame(extracted_features, columns=col_names)\n",
        "\n",
        "print(\"Funzioni di preprocessing definite correttamente.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "def get_model(input_dim):\n",
        "    \"\"\"\n",
        "    Crea una rete neurale densa per regressione.\n",
        "    \"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "\n",
        "        # Primo layer ampio\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.3), # Dropout per evitare overfitting (visto che i dati sono pochi per client)\n",
        "\n",
        "        # Secondo layer\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.2),\n",
        "\n",
        "        # Terzo layer\n",
        "        layers.Dense(32, activation='relu'),\n",
        "\n",
        "        # Output Layer (1 neurone lineare per predire 0-100)\n",
        "        layers.Dense(1, activation='linear')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "print(\"Modello definito.\")"
      ],
      "metadata": {
        "id": "3AVr1twqsd0O",
        "outputId": "d1551d1b-4025-481f-e398-61c550787f52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello definito.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIGURAZIONE ---\n",
        "TRAIN_ROOT = \"CSV_train\"\n",
        "ROUNDS = 15\n",
        "EPOCHS_PER_CLIENT = 3\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def run_federated_learning_grouped():\n",
        "    # 1. Identificazione dei Client (Sottocartelle)\n",
        "    # Cerca tutte le cartelle dentro CSV_train (es. CSV_train/Group_1)\n",
        "    # Se CSV_train contiene direttamente i csv, adattiamo la logica,\n",
        "    # ma assumiamo la struttura a sottocartelle descritta.\n",
        "\n",
        "    # Trova le sottocartelle\n",
        "    subfolders = [f.path for f in os.scandir(TRAIN_ROOT) if f.is_dir()]\n",
        "\n",
        "    # Se non ci sono sottocartelle, prova a vedere se c'è un livello intermedio\n",
        "    # (A volte gli zip creano CSV_train/CSV_train/...)\n",
        "    if not subfolders:\n",
        "        # Fallback: trattiamo la cartella root come un unico client se non ci sono gruppi\n",
        "        # Ma l'obiettivo è avere più gruppi.\n",
        "        print(\"Nessuna sottocartella trovata direttamente. Controllo ricorsivo...\")\n",
        "        # Logica semplice: ogni cartella che contiene csv è un client\n",
        "        client_folders = []\n",
        "        for root, dirs, files in os.walk(TRAIN_ROOT):\n",
        "            csv_files = [f for f in files if f.endswith('.csv')]\n",
        "            if len(csv_files) > 0:\n",
        "                client_folders.append(root)\n",
        "    else:\n",
        "        client_folders = subfolders\n",
        "\n",
        "    print(f\"Trovati {len(client_folders)} Client (Gruppi):\")\n",
        "    for c in client_folders: print(f\" - {os.path.basename(c)}\")\n",
        "\n",
        "    if not client_folders:\n",
        "        print(\"ERRORE CRITICO: Nessun dato trovato.\")\n",
        "        return None, None\n",
        "\n",
        "    # 2. Inizializzazione Modello Globale\n",
        "    # Leggiamo un file a caso per le dimensioni\n",
        "    first_csv = [f for f in os.listdir(client_folders[0]) if f.endswith('.csv')][0]\n",
        "    sample_df = pd.read_csv(os.path.join(client_folders[0], first_csv), sep=';')\n",
        "    sample_X = process_dataframe(sample_df.head(5))\n",
        "    input_dim = sample_X.shape[1]\n",
        "\n",
        "    global_model = get_model(input_dim)\n",
        "    global_weights = global_model.get_weights()\n",
        "\n",
        "    # --- LOOP FEDERATO ---\n",
        "    for round_num in range(ROUNDS):\n",
        "        print(f\"\\n--- Round {round_num + 1}/{ROUNDS} ---\")\n",
        "        local_weights_list = []\n",
        "        local_losses = []\n",
        "\n",
        "        for client_folder in client_folders:\n",
        "            try:\n",
        "                # A. AGGREGAZIONE DATI DEL GRUPPO (CLIENT)\n",
        "                # Uniamo tutti i CSV di questo gruppo in un unico DataFrame\n",
        "                group_csvs = [os.path.join(client_folder, f) for f in os.listdir(client_folder) if f.endswith('.csv')]\n",
        "\n",
        "                df_list = []\n",
        "                for csv_file in group_csvs:\n",
        "                    df_temp = pd.read_csv(csv_file, sep=';')\n",
        "                    df_list.append(df_temp)\n",
        "\n",
        "                if not df_list: continue\n",
        "\n",
        "                df_client = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "                # B. PREPROCESSING\n",
        "                X_client_extracted = process_dataframe(df_client)\n",
        "                y_client = df_client['label'].values\n",
        "\n",
        "                # C. SPLIT TRAIN / VALIDATION (Locale al client)\n",
        "                # Parte dei dati usata per il training, parte per validare\n",
        "                X_train, X_val, y_train, y_val = train_test_split(X_client_extracted, y_client, test_size=0.2, random_state=42)\n",
        "\n",
        "                # D. NORMALIZZAZIONE (StandardScaler Locale)\n",
        "                scaler = StandardScaler()\n",
        "                X_train_scaled = scaler.fit_transform(X_train)\n",
        "                X_val_scaled = scaler.transform(X_val) # Usa lo scaler del train\n",
        "\n",
        "                # E. TRAINING LOCALE\n",
        "                client_model = get_model(input_dim)\n",
        "                client_model.set_weights(global_weights)\n",
        "\n",
        "                # Early Stopping sulla Validation Loss locale\n",
        "                es = callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "                history = client_model.fit(\n",
        "                    X_train_scaled, y_train,\n",
        "                    validation_data=(X_val_scaled, y_val),\n",
        "                    epochs=EPOCHS_PER_CLIENT,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    verbose=0,\n",
        "                    callbacks=[es]\n",
        "                )\n",
        "\n",
        "                # Salviamo i pesi e la loss finale\n",
        "                local_weights_list.append(client_model.get_weights())\n",
        "                val_loss = history.history['val_loss'][-1]\n",
        "                local_losses.append(val_loss)\n",
        "\n",
        "                # print(f\"   Client {os.path.basename(client_folder)} -> Val Loss: {val_loss:.2f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   Errore Client {os.path.basename(client_folder)}: {e}\")\n",
        "\n",
        "        # F. AGGREGAZIONE (FedAvg)\n",
        "        if local_weights_list:\n",
        "            avg_weights = []\n",
        "            for layer_idx in range(len(global_weights)):\n",
        "                layer_weights = [w[layer_idx] for w in local_weights_list]\n",
        "                avg_weights.append(np.mean(layer_weights, axis=0))\n",
        "\n",
        "            global_weights = avg_weights\n",
        "            global_model.set_weights(global_weights)\n",
        "\n",
        "            avg_loss = np.mean(local_losses)\n",
        "            print(f\"   >>> Round completato. Avg Global Validation Loss: {avg_loss:.2f}\")\n",
        "\n",
        "    return global_model, input_dim\n",
        "\n",
        "# AVVIO\n",
        "final_model, feature_dim = run_federated_learning_grouped()"
      ],
      "metadata": {
        "id": "HIlvq3iZtofL",
        "outputId": "186c6eb9-d538-4dcd-8db9-e84791c8e175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1990760183.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m# AVVIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m \u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_federated_learning_grouped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1990760183.py\u001b[0m in \u001b[0;36mrun_federated_learning_grouped\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Trova le sottocartelle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0msubfolders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_ROOT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Se non ci sono sottocartelle, prova a vedere se c'è un livello intermedio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if final_model:\n",
        "    print(\"\\nGenerazione predizioni per x_test.csv...\")\n",
        "    df_test = pd.read_csv(\"x_test.csv\", sep=';')\n",
        "\n",
        "    # Feature Extraction\n",
        "    X_test_extracted = process_dataframe(df_test)\n",
        "\n",
        "    # Normalizzazione (Fit sul test set per adattamento dominio semplice)\n",
        "    scaler_test = StandardScaler()\n",
        "    X_test_scaled = scaler_test.fit_transform(X_test_extracted)\n",
        "\n",
        "    # Predizione\n",
        "    predictions = final_model.predict(X_test_scaled)\n",
        "\n",
        "    # Creazione CSV\n",
        "    submission = pd.DataFrame({\n",
        "        'id': df_test['id'],\n",
        "        'label': predictions.flatten()\n",
        "    })\n",
        "\n",
        "    submission.to_csv('submission.csv', index=False)\n",
        "    print(\"File 'submission.csv' pronto per il download!\")\n",
        "    print(submission.head())\n",
        "else:\n",
        "    print(\"Training fallito, nessun modello generato.\")"
      ],
      "metadata": {
        "id": "jnYY55Sotrga"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}