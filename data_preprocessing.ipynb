{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPTQeqk2gx8Oj8IBTL5rj/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Giuse1093/CSI_Project4/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installazione librerie (se mancano)\n",
        "!pip install -q flwr tensorflow pandas numpy scipy scikit-learn\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Decomprimiamo il file di training se non esiste già la cartella\n",
        "if not os.path.exists(\"CSV_train\") and os.path.exists(\"CSV_train.zip\"):\n",
        "    print(\"Decompressione CSV_train.zip in corso...\")\n",
        "    with zipfile.ZipFile(\"CSV_train.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"CSV_train\")\n",
        "    print(\"Fatto!\")\n",
        "elif not os.path.exists(\"CSV_train.zip\"):\n",
        "    print(\"ATTENZIONE: Carica il file 'CSV_train.zip' su Colab!\")\n",
        "else:\n",
        "    print(\"Cartella CSV_train già presente.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGpZiPJgyhIB",
        "outputId": "3ff28ec0-a848-4862-fb01-2813366e9050"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cartella CSV_train già presente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lIHO5InzaI5Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59629eb4-e5b2-47c0-b165-058e18eeda0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Funzioni di preprocessing definite correttamente.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "from scipy.fft import fft\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "\n",
        "# Ignoriamo warning minori di pandas\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Colonne contenenti le serie temporali\n",
        "TS_COLS = ['hr_time_series', 'resp_time_series', 'stress_time_series']\n",
        "\n",
        "def clean_series_interpolation(series_str):\n",
        "    \"\"\"\n",
        "    Replica la logica di 'Accelerometer_Preprocessing.ipynb':\n",
        "    1. Parsing da stringa a lista.\n",
        "    2. Sostituzione valori errati (<=0) con NaN.\n",
        "    3. Interpolazione Lineare per riempire i buchi mantenendo il trend temporale.\n",
        "    \"\"\"\n",
        "    if not isinstance(series_str, str):\n",
        "        return np.array([])\n",
        "\n",
        "    try:\n",
        "        # 1. Parsing\n",
        "        data = np.array(ast.literal_eval(series_str), dtype=float)\n",
        "\n",
        "        # 2. Rimuovi outlier/errori sensore (-1, -2, 0) impostandoli a NaN\n",
        "        data[data <= 0] = np.nan\n",
        "\n",
        "        # Trasforma in Series per usare interpolate\n",
        "        s_data = pd.Series(data)\n",
        "\n",
        "        # 3. Interpolazione (Linear) - Fondamentale per non perdere la sequenza\n",
        "        s_data = s_data.interpolate(method='linear', limit_direction='both')\n",
        "\n",
        "        # Se restano NaN (es. serie vuota), riempi con 0\n",
        "        s_data = s_data.fillna(0)\n",
        "\n",
        "        return s_data.values\n",
        "    except:\n",
        "        return np.array([])\n",
        "\n",
        "def extract_features_datasets_py(clean_data):\n",
        "    \"\"\"\n",
        "    Replica ESATTA di 'datasets.py' -> 'calculate_params':\n",
        "    Estrae: Mean, Std, Percentili (25, 50, 75), Energia Spettrale.\n",
        "    \"\"\"\n",
        "    if len(clean_data) == 0:\n",
        "        return [0.0] * 6\n",
        "\n",
        "    # Statistiche base\n",
        "    mean = np.mean(clean_data)\n",
        "    std = np.std(clean_data)\n",
        "    p25 = np.percentile(clean_data, 25)\n",
        "    p50 = np.percentile(clean_data, 50) # Mediana\n",
        "    p75 = np.percentile(clean_data, 75)\n",
        "\n",
        "    # Energia Spettrale (FFT) - Come nel file della prof\n",
        "    f_transform = fft(clean_data)\n",
        "    spectral_energy = np.sum(np.abs(f_transform)**2) / len(clean_data)\n",
        "\n",
        "    return [mean, std, p25, p50, p75, spectral_energy]\n",
        "\n",
        "def process_dataframe(df):\n",
        "    \"\"\"\n",
        "    Prende il dataframe grezzo e restituisce un dataframe di feature numeriche.\n",
        "    \"\"\"\n",
        "    extracted_features = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        row_feats = []\n",
        "        for col in TS_COLS:\n",
        "            # A. Pulizia\n",
        "            clean_seq = clean_series_interpolation(row[col])\n",
        "            # B. Estrazione Feature\n",
        "            feats = extract_features_datasets_py(clean_seq)\n",
        "            row_feats.extend(feats)\n",
        "\n",
        "        extracted_features.append(row_feats)\n",
        "\n",
        "    # Creiamo nomi colonne parlanti\n",
        "    col_names = []\n",
        "    prefixes = ['hr', 'resp', 'stress']\n",
        "    suffixes = ['mean', 'std', 'p25', 'p50', 'p75', 'energy']\n",
        "    for p in prefixes:\n",
        "        for s in suffixes:\n",
        "            col_names.append(f\"{p}_{s}\")\n",
        "\n",
        "    return pd.DataFrame(extracted_features, columns=col_names)\n",
        "\n",
        "print(\"Funzioni di preprocessing definite correttamente.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "def get_model(input_dim):\n",
        "    \"\"\"\n",
        "    Crea una rete neurale densa per regressione.\n",
        "    \"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "\n",
        "        # Primo layer ampio\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.3), # Dropout per evitare overfitting (visto che i dati sono pochi per client)\n",
        "\n",
        "        # Secondo layer\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.2),\n",
        "\n",
        "        # Terzo layer\n",
        "        layers.Dense(32, activation='relu'),\n",
        "\n",
        "        # Output Layer (1 neurone lineare per predire 0-100)\n",
        "        layers.Dense(1, activation='linear')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "print(\"Modello definito.\")"
      ],
      "metadata": {
        "id": "3AVr1twqsd0O",
        "outputId": "cab66e6b-0705-49c1-c4bd-6f022da6630c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello definito.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELLA 4: FEDERATED LEARNING (CORRETTA) ---\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split  # <--- ECCO L'IMPORT CHE MANCAVA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "# --- CONFIGURAZIONE ---\n",
        "TRAIN_ROOT = \"CSV_train\"\n",
        "ROUNDS = 15\n",
        "EPOCHS_PER_CLIENT = 3\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def run_federated_learning_smart():\n",
        "    # 1. SCOPERTA AUTOMATICA DEI GRUPPI (CLIENT)\n",
        "    client_folders = []\n",
        "    for root, dirs, files in os.walk(TRAIN_ROOT):\n",
        "        csv_files = [f for f in files if f.endswith('.csv') and 'train' in f]\n",
        "        if len(csv_files) > 0:\n",
        "            client_folders.append(root)\n",
        "\n",
        "    if not client_folders:\n",
        "        print(\"❌ ERRORE: Nessun file .csv di training trovato! Verifica il file zip.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"✅ Trovati {len(client_folders)} Gruppi/Client:\")\n",
        "    for c in client_folders:\n",
        "        n_files = len([f for f in os.listdir(c) if f.endswith('.csv')])\n",
        "        print(f\"   - {c} ({n_files} file csv)\")\n",
        "\n",
        "    # 2. INIZIALIZZAZIONE MODELLO GLOBALE\n",
        "    # Usiamo il primo file trovato per capire quante feature abbiamo\n",
        "    first_folder = client_folders[0]\n",
        "    first_csv = [f for f in os.listdir(first_folder) if f.endswith('.csv')][0]\n",
        "\n",
        "    print(\"\\nAnalisi dimensioni input...\")\n",
        "    sample_df = pd.read_csv(os.path.join(first_folder, first_csv), sep=';')\n",
        "    # Nota: process_dataframe deve essere stata definita nella Cella 2\n",
        "    sample_X = process_dataframe(sample_df.head(5))\n",
        "    input_dim = sample_X.shape[1]\n",
        "    print(f\"Dimensioni Feature Input: {input_dim}\")\n",
        "\n",
        "    global_model = get_model(input_dim) # Nota: get_model deve essere definita nella Cella 3\n",
        "    global_weights = global_model.get_weights()\n",
        "\n",
        "    # --- LOOP FEDERATO ---\n",
        "    print(\"\\nInizio Training Federato...\")\n",
        "    for round_num in range(ROUNDS):\n",
        "        print(f\"\\n--- Round {round_num + 1}/{ROUNDS} ---\")\n",
        "        local_weights_list = []\n",
        "        local_losses = []\n",
        "\n",
        "        for client_folder in client_folders:\n",
        "            try:\n",
        "                # A. CARICAMENTO DATI GRUPPO\n",
        "                group_csvs = [os.path.join(client_folder, f) for f in os.listdir(client_folder) if f.endswith('.csv')]\n",
        "\n",
        "                df_list = []\n",
        "                for csv_path in group_csvs:\n",
        "                    try:\n",
        "                        df_temp = pd.read_csv(csv_path, sep=';')\n",
        "                        df_list.append(df_temp)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                if not df_list: continue\n",
        "\n",
        "                # Unisce i dati di tutti gli utenti del gruppo\n",
        "                df_client = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "                # B. PREPROCESSING\n",
        "                X_client = process_dataframe(df_client)\n",
        "                if 'label' not in df_client.columns: continue\n",
        "                y_client = df_client['label'].values\n",
        "\n",
        "                # C. SPLIT TRAIN/VAL LOCALE\n",
        "                # Se abbiamo pochi dati, saltiamo questo client per evitare errori\n",
        "                if len(X_client) < 10:\n",
        "                    continue\n",
        "\n",
        "                # *** QUI SI VERIFICAVA L'ERRORE ***\n",
        "                X_train, X_val, y_train, y_val = train_test_split(X_client, y_client, test_size=0.2, random_state=42)\n",
        "\n",
        "                # D. NORMALIZZAZIONE (StandardScaler Locale)\n",
        "                scaler = StandardScaler()\n",
        "                X_train_scaled = scaler.fit_transform(X_train)\n",
        "                X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "                # E. TRAINING LOCALE\n",
        "                client_model = get_model(input_dim)\n",
        "                client_model.set_weights(global_weights)\n",
        "\n",
        "                es = callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "                history = client_model.fit(\n",
        "                    X_train_scaled, y_train,\n",
        "                    validation_data=(X_val_scaled, y_val),\n",
        "                    epochs=EPOCHS_PER_CLIENT,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    verbose=0,\n",
        "                    callbacks=[es]\n",
        "                )\n",
        "\n",
        "                local_weights_list.append(client_model.get_weights())\n",
        "                local_losses.append(history.history['val_loss'][-1])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   Errore nel gruppo {os.path.basename(client_folder)}: {e}\")\n",
        "\n",
        "        # F. AGGREGAZIONE (FEDAVG)\n",
        "        if local_weights_list:\n",
        "            avg_weights = []\n",
        "            for layer_idx in range(len(global_weights)):\n",
        "                layer_weights = [w[layer_idx] for w in local_weights_list]\n",
        "                avg_weights.append(np.mean(layer_weights, axis=0))\n",
        "\n",
        "            global_weights = avg_weights\n",
        "            global_model.set_weights(global_weights)\n",
        "\n",
        "            avg_loss = np.mean(local_losses)\n",
        "            print(f\"   >>> Loss di Validazione Media (Global): {avg_loss:.2f}\")\n",
        "        else:\n",
        "            print(\"   Nessun client ha completato il training in questo round.\")\n",
        "\n",
        "    return global_model\n",
        "\n",
        "# ESECUZIONE\n",
        "final_model = run_federated_learning_smart()"
      ],
      "metadata": {
        "id": "QRj9fLjV0ljc",
        "outputId": "db37ff02-cda1-4185-aaf9-41a0981885ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Trovati 9 Gruppi/Client:\n",
            "   - CSV_train/CSV_train/group1 (5 file csv)\n",
            "   - CSV_train/CSV_train/group3 (5 file csv)\n",
            "   - CSV_train/CSV_train/group4 (5 file csv)\n",
            "   - CSV_train/CSV_train/group2 (5 file csv)\n",
            "   - CSV_train/CSV_train/group0 (5 file csv)\n",
            "   - CSV_train/CSV_train/group6 (5 file csv)\n",
            "   - CSV_train/CSV_train/group7 (5 file csv)\n",
            "   - CSV_train/CSV_train/group5 (5 file csv)\n",
            "   - CSV_train/CSV_train/group8 (5 file csv)\n",
            "\n",
            "Analisi dimensioni input...\n",
            "Dimensioni Feature Input: 18\n",
            "\n",
            "Inizio Training Federato...\n",
            "\n",
            "--- Round 1/15 ---\n",
            "   >>> Loss di Validazione Media (Global): 5786.79\n",
            "\n",
            "--- Round 2/15 ---\n",
            "   >>> Loss di Validazione Media (Global): 5197.73\n",
            "\n",
            "--- Round 3/15 ---\n",
            "   >>> Loss di Validazione Media (Global): 4049.89\n",
            "\n",
            "--- Round 4/15 ---\n",
            "   >>> Loss di Validazione Media (Global): 2427.88\n",
            "\n",
            "--- Round 5/15 ---\n",
            "   >>> Loss di Validazione Media (Global): 1304.96\n",
            "\n",
            "--- Round 6/15 ---\n",
            "   >>> Loss di Validazione Media (Global): 974.91\n",
            "\n",
            "--- Round 7/15 ---\n",
            "   >>> Loss di Validazione Media (Global): 857.84\n",
            "\n",
            "--- Round 8/15 ---\n",
            "   >>> Loss di Validazione Media (Global): 815.10\n",
            "\n",
            "--- Round 9/15 ---\n",
            "   >>> Loss di Validazione Media (Global): 741.84\n",
            "\n",
            "--- Round 10/15 ---\n",
            "   >>> Loss di Validazione Media (Global): 720.75\n",
            "\n",
            "--- Round 11/15 ---\n",
            "   >>> Loss di Validazione Media (Global): 680.79\n",
            "\n",
            "--- Round 12/15 ---\n",
            "   >>> Loss di Validazione Media (Global): 672.67\n",
            "\n",
            "--- Round 13/15 ---\n",
            "   >>> Loss di Validazione Media (Global): 632.00\n",
            "\n",
            "--- Round 14/15 ---\n",
            "   >>> Loss di Validazione Media (Global): 621.33\n",
            "\n",
            "--- Round 15/15 ---\n",
            "   >>> Loss di Validazione Media (Global): 616.29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if final_model:\n",
        "    print(\"\\nGenerazione predizioni per x_test.csv...\")\n",
        "    df_test = pd.read_csv(\"x_test.csv\", sep=';')\n",
        "\n",
        "    # Feature Extraction\n",
        "    X_test_extracted = process_dataframe(df_test)\n",
        "\n",
        "    # Normalizzazione (Fit sul test set per adattamento dominio semplice)\n",
        "    scaler_test = StandardScaler()\n",
        "    X_test_scaled = scaler_test.fit_transform(X_test_extracted)\n",
        "\n",
        "    # Predizione\n",
        "    predictions = final_model.predict(X_test_scaled)\n",
        "\n",
        "    # Creazione CSV\n",
        "    submission = pd.DataFrame({\n",
        "        'id': df_test['id'],\n",
        "        'label': predictions.flatten()\n",
        "    })\n",
        "\n",
        "    submission.to_csv('submission.csv', index=False)\n",
        "    print(\"File 'submission.csv' pronto per il download!\")\n",
        "    print(submission.head())\n",
        "else:\n",
        "    print(\"Training fallito, nessun modello generato.\")"
      ],
      "metadata": {
        "id": "jnYY55Sotrga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02e31a80-638e-457a-e269-995e832fd35a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generazione predizioni per x_test.csv...\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "File 'submission.csv' pronto per il download!\n",
            "   id      label\n",
            "0   0  84.308388\n",
            "1   1  72.784363\n",
            "2   2  72.416115\n",
            "3   3  77.850410\n",
            "4   4  75.255302\n"
          ]
        }
      ]
    }
  ]
}