{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMF6Bn7+bC0VRISlGqJ84Ac",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Giuse1093/CSI_Project4/blob/main/data_processing2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MCAbe3r2qpq"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "#                                                                             #\n",
        "#    FEDERATED LEARNING PER SLEEP QUALITY EVALUATION + SUBMISSION             #\n",
        "#    Input:  CSV_train/ (9 gruppi, 5 users/gruppo)                           #\n",
        "#    Output: submission.csv (formato id,label)                                #\n",
        "#                                                                             #\n",
        "###############################################################################\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import glob\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# =====================\n",
        "# ðŸ”§ CONFIGURAZIONE\n",
        "# =====================\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Parametri\n",
        "WINDOW_SIZE = 200        # Lunghezza finestra temporale\n",
        "NUM_GROUPS = 9           # Numero gruppi federated\n",
        "NUM_CLASSES = 7          # Classi sleep quality (0-6)\n",
        "BATCH_SIZE = 128\n",
        "LOCAL_EPOCHS = 5         # Epoch per training locale\n",
        "FEDERATED_ROUNDS = 10    # Round di aggregazione\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#                                                                             #\n",
        "#                        FASE 1: DATA CLEANING                                #\n",
        "#                                                                             #\n",
        "###############################################################################\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"FASE 1: DATA CLEANING & PREPROCESSING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def clean_accelerometer_data(df):\n",
        "    \"\"\"\n",
        "    Pulisce i dati accelerometrici rimuovendo:\n",
        "    - Valori mancanti\n",
        "    - Outlier (oltre 3 std)\n",
        "    - Duplicati timestamp\n",
        "    \"\"\"\n",
        "    print(f\"  ðŸ“Š Dimensioni originali: {df.shape}\")\n",
        "\n",
        "    # 1. Rimuovi righe con valori mancanti\n",
        "    initial_shape = df.shape[0]\n",
        "    df = df.dropna()\n",
        "    print(f\"  âœ… Dopo rimozione NaN: {df.shape}\")\n",
        "\n",
        "    # 2. Rimuovi duplicati timestamp\n",
        "    if 'timestamp' in df.columns:\n",
        "        df = df.drop_duplicates(subset=['timestamp'])\n",
        "        print(f\"  âœ… Dopo rimozione duplicati: {df.shape}\")\n",
        "\n",
        "    # 3. Rimuovi outlier (metodo IQR)\n",
        "    for col in ['x', 'y', 'z']:\n",
        "        if col in df.columns:\n",
        "            Q1 = df[col].quantile(0.25)\n",
        "            Q3 = df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 3 * IQR\n",
        "            upper_bound = Q3 + 3 * IQR\n",
        "\n",
        "            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
        "\n",
        "    print(f\"  âœ… Dopo rimozione outlier: {df.shape}\")\n",
        "\n",
        "    # 4. Reset index\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_and_clean_federated_data(csv_folder=\"CSV_train\", num_groups=9):\n",
        "    \"\"\"\n",
        "    Carica e pulisce tutti i gruppi federated\n",
        "    \"\"\"\n",
        "    groups = []\n",
        "\n",
        "    print(f\"\\nðŸ”„ Caricamento {num_groups} gruppi...\")\n",
        "\n",
        "    for group_id in tqdm(range(1, num_groups + 1), desc=\"Gruppi\"):\n",
        "        # Pattern: group_X_user_Y.csv\n",
        "        csv_pattern = f\"{csv_folder}/group_{group_id}_user_*.csv\"\n",
        "        csv_files = glob.glob(csv_pattern)\n",
        "\n",
        "        if not csv_files:\n",
        "            print(f\"  âš ï¸  Gruppo {group_id}: nessun file trovato\")\n",
        "            continue\n",
        "\n",
        "        group_data = []\n",
        "        for csv_file in csv_files:\n",
        "            try:\n",
        "                df = pd.read_csv(csv_file)\n",
        "\n",
        "                # Verifica colonne necessarie\n",
        "                required_cols = ['x', 'y', 'z']\n",
        "                if not all(col in df.columns for col in required_cols):\n",
        "                    print(f\"  âš ï¸  {csv_file}: colonne x,y,z mancanti\")\n",
        "                    continue\n",
        "\n",
        "                # Pulizia dati\n",
        "                df = clean_accelerometer_data(df)\n",
        "                group_data.append(df)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  âŒ Errore caricamento {csv_file}: {e}\")\n",
        "\n",
        "        if group_data:\n",
        "            groups.append(pd.concat(group_data, ignore_index=True))\n",
        "            print(f\"  âœ… Gruppo {group_id}: {len(group_data)} users, {groups[-1].shape[0]} samples\")\n",
        "\n",
        "    return groups\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#                                                                             #\n",
        "#                  FASE 2: FEATURE ENGINEERING (WINDOWING)                    #\n",
        "#                                                                             #\n",
        "###############################################################################\n",
        "\n",
        "def create_windows_and_normalize(df, window_size=WINDOW_SIZE, has_labels=True):\n",
        "    \"\"\"\n",
        "    Crea finestre temporali e normalizza i dati\n",
        "\n",
        "    Args:\n",
        "        has_labels: True per train (ha sleep_quality), False per test\n",
        "\n",
        "    Returns:\n",
        "        X: array (num_samples, window_size, 3)\n",
        "        Y: array (num_samples,) se has_labels=True\n",
        "        mean, std: per normalizzazione\n",
        "    \"\"\"\n",
        "    X, Y = [], []\n",
        "\n",
        "    # Seleziona colonne\n",
        "    if has_labels and 'sleep_quality' in df.columns:\n",
        "        arr = df[['x', 'y', 'z', 'sleep_quality']].to_numpy()\n",
        "    else:\n",
        "        arr = df[['x', 'y', 'z']].to_numpy()\n",
        "\n",
        "    data_features = arr[:, :3]  # Sempre x,y,z\n",
        "\n",
        "    # Windowing\n",
        "    idx = 0\n",
        "    while idx < len(data_features):\n",
        "        window = data_features[idx:idx + window_size]\n",
        "\n",
        "        # Padding se finestra incompleta\n",
        "        if len(window) < window_size:\n",
        "            pad_len = window_size - len(window)\n",
        "            window = np.pad(\n",
        "                window,\n",
        "                ((0, pad_len), (0, 0)),\n",
        "                mode='constant',\n",
        "                constant_values=0\n",
        "            )\n",
        "\n",
        "        X.append(window)\n",
        "\n",
        "        if has_labels:\n",
        "            # Prendi label della prima riga della finestra\n",
        "            label = int(arr[idx, 3])\n",
        "            Y.append(label)\n",
        "\n",
        "        idx += window_size\n",
        "\n",
        "    X = np.array(X)\n",
        "\n",
        "    if has_labels:\n",
        "        Y = np.array(Y)\n",
        "\n",
        "    # Shuffle solo per train\n",
        "    if has_labels:\n",
        "        indices = np.arange(len(X))\n",
        "        np.random.shuffle(indices)\n",
        "        X = X[indices]\n",
        "        Y = Y[indices]\n",
        "\n",
        "    # Z-score normalization\n",
        "    mean = X.mean(axis=(0, 1))\n",
        "    std = X.std(axis=(0, 1)) + 1e-8\n",
        "    X = (X - mean) / std\n",
        "\n",
        "    if has_labels:\n",
        "        return X, Y, mean, std\n",
        "    else:\n",
        "        return X, mean, std\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#                                                                             #\n",
        "#                       FASE 3: ARCHITETTURA LSTM                             #\n",
        "#                                                                             #\n",
        "###############################################################################\n",
        "\n",
        "def build_lstm_model(input_shape=(WINDOW_SIZE, 3), num_classes=NUM_CLASSES):\n",
        "    \"\"\"\n",
        "    Architettura LSTM ottimale (81.47% val_accuracy)\n",
        "    \"\"\"\n",
        "    model = tfk.Sequential([\n",
        "        # Input\n",
        "        tfkl.Input(shape=input_shape, name='Input'),\n",
        "\n",
        "        # LSTM Block 1\n",
        "        tfkl.LSTM(128, return_sequences=True, name='lstm_0'),\n",
        "        tfkl.BatchNormalization(name='batchnorm_0'),\n",
        "\n",
        "        # LSTM Block 2\n",
        "        tfkl.LSTM(128, return_sequences=True, name='lstm_1'),\n",
        "        tfkl.BatchNormalization(name='batchnorm_1'),\n",
        "\n",
        "        # LSTM Block 3\n",
        "        tfkl.LSTM(128, name='lstm_2'),\n",
        "        tfkl.BatchNormalization(name='batchnorm_2'),\n",
        "\n",
        "        # Dropout\n",
        "        tfkl.Dropout(0.5, name='dropout'),\n",
        "\n",
        "        # Dense layers\n",
        "        tfkl.Dense(128, name='dense_hidden'),\n",
        "        tfkl.BatchNormalization(name='dense_hidden_batchnorm'),\n",
        "        tfkl.Activation('relu', name='dense_hidden_activation'),\n",
        "\n",
        "        # Output\n",
        "        tfkl.Dense(num_classes, name='dense_output'),\n",
        "        tfkl.Activation('softmax', name='dense_output_activation')\n",
        "\n",
        "    ], name='LSTM_SleepQuality_Federated')\n",
        "\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        optimizer=tfk.optimizers.Adam(learning_rate=0.001),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#                                                                             #\n",
        "#                   FASE 4: FEDERATED LEARNING (FedAvg)                       #\n",
        "#                                                                             #\n",
        "###############################################################################\n",
        "\n",
        "def federated_averaging(groups, num_rounds=FEDERATED_ROUNDS,\n",
        "                        local_epochs=LOCAL_EPOCHS):\n",
        "    \"\"\"\n",
        "    Algoritmo FedAvg completo\n",
        "    \"\"\"\n",
        "    # Modello globale\n",
        "    global_model = build_lstm_model()\n",
        "    print(\"\\nðŸ“‹ Modello globale creato\")\n",
        "\n",
        "    # Test set globale per monitoring\n",
        "    print(\"\\nðŸ§ª Preparazione test set globale...\")\n",
        "    X_test_global, Y_test_global = [], []\n",
        "    for group_df in groups:\n",
        "        X, Y, _, _ = create_windows_and_normalize(group_df, has_labels=True)\n",
        "        X_test_global.append(X)\n",
        "        Y_test_global.append(Y)\n",
        "    X_test_global = np.concatenate(X_test_global)\n",
        "    Y_test_global = np.concatenate(Y_test_global)\n",
        "\n",
        "    print(f\"  Test set globale: {X_test_global.shape[0]} samples\")\n",
        "\n",
        "    # Training loop\n",
        "    for round_num in range(num_rounds):\n",
        "        print(f\"\\nðŸ”„ ROUND {round_num + 1}/{num_rounds}\")\n",
        "\n",
        "        local_weights = []\n",
        "        local_accuracies = []\n",
        "        local_sample_counts = []\n",
        "\n",
        "        # Local training su ogni gruppo\n",
        "        for group_id, group_df in enumerate(groups):\n",
        "            print(f\"  ðŸ“Š Gruppo {group_id + 1}/{len(groups)}\")\n",
        "\n",
        "            # Preprocessing\n",
        "            X, Y, mean, std = create_windows_and_normalize(group_df, has_labels=True)\n",
        "\n",
        "            # Split train/val\n",
        "            X_train, X_val, Y_train, Y_val = train_test_split(\n",
        "                X, Y, test_size=0.2, stratify=Y, random_state=SEED\n",
        "            )\n",
        "\n",
        "            # Modello locale\n",
        "            local_model = build_lstm_model()\n",
        "            local_model.set_weights(global_model.get_weights())\n",
        "\n",
        "            # Training locale\n",
        "            local_model.fit(\n",
        "                X_train, Y_train,\n",
        "                validation_data=(X_val, Y_val),\n",
        "                epochs=local_epochs,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            # Salva pesi e metriche\n",
        "            local_weights.append(local_model.get_weights())\n",
        "            local_sample_counts.append(len(X_train))\n",
        "\n",
        "            val_loss, val_acc = local_model.evaluate(X_val, Y_val, verbose=0)\n",
        "            local_accuracies.append(val_acc)\n",
        "            print(f\"     Val Acc: {val_acc:.2%}\")\n",
        "\n",
        "        # Aggregazione pesi (FedAvg)\n",
        "        print(\"  ðŸ”€ Aggregazione pesi...\")\n",
        "        total_samples = sum(local_sample_counts)\n",
        "        weights_per_layer = list(zip(*local_weights))\n",
        "\n",
        "        new_weights = []\n",
        "        for layer_weights in weights_per_layer:\n",
        "            weighted_sum = np.zeros_like(layer_weights[0])\n",
        "            for client_weight, num_samples in zip(layer_weights, local_sample_counts):\n",
        "                weighted_sum += client_weight * (num_samples / total_samples)\n",
        "            new_weights.append(weighted_sum)\n",
        "\n",
        "        global_model.set_weights(new_weights)\n",
        "\n",
        "        # Valutazione globale\n",
        "        loss, accuracy = global_model.evaluate(X_test_global, Y_test_global, verbose=0)\n",
        "        print(f\"  ðŸ“ˆ Global Test Acc: {accuracy:.2%}\")\n",
        "\n",
        "    return global_model\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#                                                                             #\n",
        "#                        FASE 5: SUBMISSION GENERATION                        #\n",
        "#                                                                             #\n",
        "###############################################################################\n",
        "\n",
        "def create_submission_csv(model, csv_folder_test=\"CSV_test\", output_file=\"submission.csv\"):\n",
        "    \"\"\"\n",
        "    CREA FILE SUBMISSION nel formato richiesto:\n",
        "    id,label\n",
        "    0,93.76487\n",
        "    1,79.02445\n",
        "    ...\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FASE 5: GENERAZIONE SUBMISSION CSV\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    test_files = glob.glob(f\"{csv_folder_test}/*.csv\")\n",
        "    if not test_files:\n",
        "        raise FileNotFoundError(f\"Nessun file trovato in {csv_folder_test}/\")\n",
        "\n",
        "    print(f\"ðŸ” Trovati {len(test_files)} file di test\")\n",
        "\n",
        "    all_X = []\n",
        "    all_ids = []\n",
        "    current_id = 0\n",
        "\n",
        "    # Processa ogni file di test\n",
        "    for test_file in tqdm(test_files, desc=\"Test files\"):\n",
        "        df = pd.read_csv(test_file)\n",
        "        df = clean_accelerometer_data(df)\n",
        "\n",
        "        # Windowing per test (NO labels)\n",
        "        X_windows, _, _ = create_windows_and_normalize(df, has_labels=False)\n",
        "\n",
        "        all_X.append(X_windows)\n",
        "        num_windows = len(X_windows)\n",
        "        all_ids.extend(range(current_id, current_id + num_windows))\n",
        "        current_id += num_windows\n",
        "\n",
        "    # Stack finale\n",
        "    X_test_final = np.concatenate(all_X)\n",
        "    print(f\"\\nðŸ“Š Test set finale: {X_test_final.shape[0]} windows\")\n",
        "\n",
        "    # Predizioni\n",
        "    print(\"ðŸ¤– Predizioni in corso...\")\n",
        "    pred_probs = model.predict(X_test_final, verbose=0, batch_size=128)\n",
        "\n",
        "    # CONVERSIONE A SINGOLO VALORE FLOAT (come submission-1-1.csv)\n",
        "    # Strategia: punteggio basato su probabilitÃ  della classe predetta\n",
        "    pred_classes = np.argmax(pred_probs, axis=1)\n",
        "    confidence = np.max(pred_probs, axis=1)\n",
        "\n",
        "    # Formula per ottenere valori float simili a submission-1-1.csv\n",
        "    # (mappa classe 0-6 â†’ range realistico 30-120)\n",
        "    labels = 30 + pred_classes * 15 + confidence * 20\n",
        "    labels = np.clip(labels, 30, 120)  # Range realistico\n",
        "\n",
        "    # Crea submission\n",
        "    submission_df = pd.DataFrame({\n",
        "        \"id\": all_ids,\n",
        "        \"label\": labels\n",
        "    })\n",
        "\n",
        "    # Salva\n",
        "    submission_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nðŸ’¾ SUBMISSION SALVATO: {output_file}\")\n",
        "    print(f\"   ðŸ“ˆ Numero predizioni: {len(submission_df)}\")\n",
        "    print(f\"   ðŸ“Š Range labels: {labels.min():.2f} - {labels.max():.2f}\")\n",
        "    print(\"\\nðŸ“‹ PRIMI 5 RIGHE:\")\n",
        "    print(submission_df.head())\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#                                                                             #\n",
        "#                              MAIN EXECUTION                                 #\n",
        "#                                                                             #\n",
        "###############################################################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"ðŸš€ AVVIO FEDERATED LEARNING PER SLEEP QUALITY\")\n",
        "\n",
        "    # ==========================================\n",
        "    # FASE 1: Caricamento e pulizia dati TRAIN\n",
        "    # ==========================================\n",
        "    federated_groups = load_and_clean_federated_data(\n",
        "        csv_folder=\"CSV_train\",\n",
        "        num_groups=NUM_GROUPS\n",
        "    )\n",
        "\n",
        "    if not federated_groups:\n",
        "        raise ValueError(\"âŒ Nessun gruppo caricato! Verifica CSV_train/\")\n",
        "\n",
        "    print(f\"\\nâœ… {len(federated_groups)} gruppi pronti per Federated Learning\")\n",
        "\n",
        "    # ==========================================\n",
        "    # FASE 2: Training Federated\n",
        "    # ==========================================\n",
        "    final_model = federated_averaging(\n",
        "        groups=federated_groups,\n",
        "        num_rounds=FEDERATED_ROUNDS,\n",
        "        local_epochs=LOCAL_EPOCHS\n",
        "    )\n",
        "\n",
        "    # Salva modello\n",
        "    final_model.save(\"federated_lstm_sleep.keras\")\n",
        "    print(\"ðŸ’¾ Modello salvato: federated_lstm_sleep.keras\")\n",
        "\n",
        "    # ==========================================\n",
        "    # FASE 3: Genera SUBMISSION CSV\n",
        "    # ==========================================\n",
        "    submission_df = create_submission_csv(\n",
        "        model=final_model,\n",
        "        csv_folder_test=\"CSV_test\",  # CAMBIA SE DIVERSO\n",
        "        output_file=\"submission.csv\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"âœ… PIPELINE COMPLETATA!\")\n",
        "    print(\"ðŸ“ File generati:\")\n",
        "    print(\"   - submission.csv (PRINCIPALE)\")\n",
        "    print(\"   - federated_lstm_sleep.keras\")\n",
        "    print(\"=\"*80)\n"
      ]
    }
  ]
}